{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af48220-e5b1-4898-bfb7-04fef1abf4d5",
   "metadata": {},
   "source": [
    "## Question - 1\n",
    "ans-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012d5b0-cdd3-4158-8090-a90e69b3507c",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale the values of a feature or dataset into a specific range, typically between 0 and 1. This transformation is useful when you want to ensure that all your data falls within a uniform range, making it easier to compare and analyze different features or datasets, particularly in machine learning and data analysis. It's done by mapping the minimum and maximum values of the original data to the desired range.\n",
    "\n",
    "\n",
    ">> Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset of exam scores, and the scores range from 60 to 95. You want to normalize these scores to a range of 0 to 1 using Min-Max scaling.\n",
    "\n",
    "* . Find the minimum and maximum values in the dataset:\n",
    "\n",
    "\n",
    "\n",
    "X min =60 (minimum score)\n",
    "\n",
    "X max =95 (maximum score)\n",
    "\n",
    "\n",
    "* . Now, let's say you have a specific exam score you want to normalize, for instance, \n",
    "\n",
    "X=80.\n",
    "\n",
    "* . Apply the Min-Max scaling formula:\n",
    "\n",
    "\n",
    "\n",
    "X norm = 95−60 / 80−60 = 20 / 35  ≈ 0.5714\n",
    "\n",
    "\n",
    "So, the normalized score for 80 in this Min-Max scaling would be approximately 0.5714.\n",
    "\n",
    "By performing Min-Max scaling on the entire dataset, you ensure that all the scores will fall within the range of 0 to 1. This can be particularly helpful in machine learning algorithms that are sensitive to the scale of the input features, as it can prevent certain features from dominating the learning process simply because they have larger numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12fb4f-528d-4b78-b74c-978a6a645a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45212112-759a-45c9-a6a0-b7af75e00f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639deba-a528-4547-a3af-f388b4d6f213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bc4f2fd-645c-4269-a0a1-435e5de91238",
   "metadata": {},
   "source": [
    "## Question - 2\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756e3e7-0b49-421f-9b26-843e548b89fe",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling, also known as \"vector normalization\" or \"scaling to unit length,\" is a data preprocessing method used to transform the values of a feature or dataset in such a way that it becomes a unit vector. A unit vector has a magnitude (length) of 1 and points in the same direction as the original vector. This technique is often used in machine learning algorithms that rely on the direction or angles between data points, such as cosine similarity or when working with Euclidean distance-based methods.\n",
    "\n",
    "The Unit Vector technique is different from Min-Max scaling, which scales data to a specific range (e.g., between 0 and 1). Unit Vector scaling doesn't focus on the range of values but rather on the direction or orientation of the data points.\n",
    "\n",
    "\n",
    "\n",
    "Suppose you have a dataset of 2D data points, and you want to calculate the unit vector for a data point \n",
    "(\n",
    "3\n",
    ",\n",
    "4\n",
    ")\n",
    "(3,4). Here's how you do it:\n",
    "\n",
    "* . Calculate the magnitude of the original vector:\n",
    "\n",
    "∣∣X∣∣= sqrt(3**2 + 4**2) = sqrt(9 + 16) \n",
    "= sqrt(25) = 5\n",
    "\n",
    "\n",
    "\n",
    "* . Calculate the unit vector:\n",
    "\n",
    "\n",
    "X unit = (3,4) /5\n",
    "\n",
    " =(3/5 , 4/5)\n",
    "\n",
    "\n",
    "* . So, the unit vector for the data point\n",
    "\n",
    "(3,4) is (3/5, 4/5). This unit vector has a magnitude of 1 and points in the same direction as the original vector.\n",
    "\n",
    "In contrast to Min-Max scaling, which changes the range of values, Unit Vector scaling preserves the direction of the data, making it particularly useful when you want to focus on the relative angles or orientations of data points rather than their magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581d773-f13b-469a-899a-929914e9b18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcad865-4e93-4162-b1ba-8bc966e6df1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4da60522-bacf-4f50-acb8-5c2090a8fb6d",
   "metadata": {},
   "source": [
    "## question - 3\n",
    "ans  - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e0fa2-3f8e-4f27-88d8-7ba81a0a018b",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used in the field of data analysis and machine learning. It helps reduce the complexity of high-dimensional data while preserving the most important information or patterns in the data. PCA accomplishes this by transforming the original data into a new set of uncorrelated variables called principal components. These principal components are linear combinations of the original features, sorted in descending order of variance, such that the first principal component captures the most variance in the data, the second captures the second most variance, and so on.\n",
    "\n",
    ">> .PCA is used for various purposes, including:\n",
    "\n",
    "* . Data Compression: By reducing the dimensionality of the data, PCA can help in data compression and storage, making it more efficient.\n",
    "\n",
    "* . Noise Reduction: PCA can help remove noise from data by focusing on the most significant features.\n",
    "\n",
    "* . Visualization: PCA can be used to visualize high-dimensional data in a lower-dimensional space, making it easier to understand and interpret.\n",
    "\n",
    "* . Feature Selection: It can assist in selecting the most important features for a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9633a6d-cdc8-42f5-8182-591ea4987b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "      Height (in inches)   Weight (in pounds)\n",
    "Person 1       68                  150\n",
    "Person 2       72                  160\n",
    "Person 3       74                  175\n",
    "Person 4       65                  125\n",
    "Person 5       60                  110\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2136d5ec-4f75-4455-8878-103dadc79ea2",
   "metadata": {},
   "source": [
    "You want to apply PCA to this dataset to reduce it to one principal component.\n",
    "\n",
    "* . Standardize the data: First, you typically standardize the data by subtracting the mean and dividing by the standard deviation of each feature.\n",
    "\n",
    "* . Calculate the covariance matrix: Next, you calculate the covariance matrix of the standardized data. This matrix represents the relationships between the features.\n",
    "\n",
    "* . Calculate the eigenvectors and eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors are the principal components, and the eigenvalues represent the amount of variance captured by each principal component.\n",
    "\n",
    "* . Select the top principal components: Sort the eigenvalues in descending order. The eigenvector corresponding to the largest eigenvalue is the first principal component.\n",
    "\n",
    "In this example, let's say the first principal component is primarily a combination of \"height\" and \"weight,\" where height contributes more. The dataset can be reduced to this single principal component, effectively reducing the dimensionality from 2 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0449507e-50c4-4b1c-bd6c-b4501053b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "   Principal Component\n",
    "Person 1       0.85\n",
    "Person 2       0.98\n",
    "Person 3       1.25\n",
    "Person 4       0.34\n",
    "Person 5      -0.78\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e34280-0fec-4ca0-a9bf-bc786a49396d",
   "metadata": {},
   "source": [
    "Now, instead of working with two features, you can work with one principal component that captures the most significant information from the original data. This simplifies data analysis, visualization, and modeling, particularly when dealing with high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15460df7-910f-4156-a2f1-5ad4174b1bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f24146-d9cd-4c62-b7a7-9f58f4d2825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8530d-4df4-4ce7-b3e4-38b7b831b5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4eea2df-77d7-4063-9340-a89f0d0d2919",
   "metadata": {},
   "source": [
    "## Question - 4\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c272738-8e56-4e5e-b0cb-e3927e6c9a6f",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique that can be used for feature extraction, and it plays a significant role in reducing the dimensionality of data while retaining important information. The relationship between PCA and feature extraction is that PCA can be applied to transform the original features into a new set of features (principal components) that are linear combinations of the original features. These principal components can be seen as a form of feature extraction because they capture the most significant patterns or information in the data.\n",
    "\n",
    "Here's how PCA is used for feature extraction:\n",
    "\n",
    "* . Data Standardization: Start by standardizing your data by subtracting the mean and dividing by the standard deviation for each feature. Standardization is important because PCA is sensitive to the scale of the features.\n",
    "\n",
    "* . Calculate Principal Components: Apply PCA to the standardized data. PCA will calculate the eigenvectors (principal components) and eigenvalues, which represent the directions and variances of the data, respectively.\n",
    "\n",
    "* . Select Principal Components: You can choose to retain a subset of the top principal components that capture a significant amount of variance in the data. For example, you might choose to keep the first k principal components, where k is typically determined based on the amount of variance you want to retain (e.g., 95% of the total variance).\n",
    "\n",
    "* . Transform Data: Use the selected principal components to transform the original data into a new feature space. Each instance in the dataset is now represented by its values along these new axes, which are linear combinations of the original features.\n",
    "\n",
    ">> .Here's an example to illustrate PCA for feature extraction:\n",
    "\n",
    "Suppose you have a dataset of images of handwritten digits (e.g., the MNIST dataset) with 784 features, where each feature represents the intensity of a pixel in a 28x28 image. You want to reduce the dimensionality of the data while preserving the most important information. You apply PCA as follows:\n",
    "\n",
    "1. Standardize the pixel values for all images.\n",
    "\n",
    "2. Apply PCA to the standardized data. PCA will compute the principal components.\n",
    "\n",
    "3. Choose to keep, for example, the first 50 principal components, which collectively capture 90% of the total variance in the data.\n",
    "\n",
    "4. Transform the data using these 50 principal components. Each image is now represented by a 50-dimensional vector instead of the original 784 dimensions.\n",
    "\n",
    "By reducing the dimensionality from 784 features to 50 features, you've performed feature extraction. The 50 new features are linear combinations of the original pixel intensities, and they capture the most important variations in the data. This reduction in dimensionality can lead to more efficient and effective machine learning models while retaining the essential information needed for tasks like digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83d4942-d3d8-401f-8d28-751f2b54baa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c86a4-2858-4ed2-aba1-e7fac8f70e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3595b11a-5671-4d8e-858b-c9ddfb1a8593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85657ad4-7284-4154-97a9-db87b0de2421",
   "metadata": {},
   "source": [
    "## Question - 5\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4b0b0d-1c90-46a0-ae53-77e6178c3dea",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling to normalize the features like price, rating, and delivery time. Min-Max scaling will transform these features to a common range (typically between 0 and 1) so that they have equal importance when you're making recommendations. Here's how you can use Min-Max scaling for each feature:\n",
    "\n",
    "1. Price: Normalize the price feature to be in the range [0, 1]. Suppose the original prices of food items vary from $5 to $20.\n",
    "\n",
    "* . Find the minimum price (X min) in the dataset, which is $5.\n",
    "\n",
    "* . Find the maximum price (X max) in the dataset, which is $20.\n",
    "\n",
    "\n",
    "* . Now, for a specific food item with a price of $15, you can apply Min-Max scaling:\n",
    "\n",
    "\n",
    "normalized = (Xmax − X) min / (X − Xmin)\n",
    "= 20−5 / 15−5\n",
    "=0.7143\n",
    "\n",
    "So, the normalized price for this item is approximately 0.7143.\n",
    "\n",
    "2. Rating: Normalize the rating feature to be in the range [0, 1]. Suppose the original ratings of restaurants vary from 2 to 5 (assuming a 5-star rating system).\n",
    "\n",
    "* .Find the minimum rating (Xmin) in the dataset, which is 2.\n",
    "\n",
    "* .Find the maximum rating (Xmax) in the dataset, which is 5.\n",
    "\n",
    "For a restaurant with a rating of 4.5, you can apply Min-Max scaling\n",
    "\n",
    "normalized = (Xmax −Xmin) / (X−Xmin)\n",
    "= (5−2) / (4.5−2)\n",
    "\n",
    "=0.75\n",
    "\n",
    "So, the normalized rating for this restaurant is 0.75.\n",
    "\n",
    "\n",
    "3. Delivery Time: Normalize the delivery time feature to be in the range [0, 1]. Suppose the original delivery times vary from 20 minutes to 60 minutes.\n",
    "\n",
    "* .Find the minimum delivery time (xmin) in the dataset, which is 20 minutes.\n",
    "\n",
    "* .Find the maximum delivery time (Xmax) in the dataset, which is 60 minutes.\n",
    "\n",
    "\n",
    "For a restaurant with a delivery time of 40 minutes, you can apply Min-Max scaling:\n",
    "\n",
    " \n",
    "normalized = (Xmax − Xmin)/(X−Xmin)\n",
    "= (60−20) / (40-20)\n",
    "=0.5\n",
    "\n",
    "So, the normalized delivery time for this restaurant is 0.5.\n",
    "\n",
    "By applying Min-Max scaling to these features, you ensure that each feature has been transformed to the same scale (0 to 1), making them comparable and suitable for use in a recommendation system. The scaled features can be used to calculate recommendations or rankings for food items or restaurants based on customer preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8885a-138d-4d88-b664-573b23df5de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca3dc1-9167-49a6-b8d8-77d58ce17d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7fd80-e83a-4ef7-ae78-8f8a4acc33f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da09a67c-f80f-4a5d-be2b-774158d522f7",
   "metadata": {},
   "source": [
    "## Question - 6\n",
    "ans  - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d4f75-835d-47df-9422-e3bfefe4ec50",
   "metadata": {},
   "source": [
    "Using PCA (Principal Component Analysis) to reduce the dimensionality of a dataset for predicting stock prices can be a beneficial approach, especially when dealing with a large number of features. Reducing dimensionality can simplify the modeling process, remove multicollinearity, and potentially improve the model's performance. Here's how you can use PCA for dimensionality reduction in the context of predicting stock prices:\n",
    "\n",
    "* .Data Preprocessing:\n",
    "\n",
    "Begin by gathering your dataset, which includes various features such as company financial data and market trends. Make sure that the data is properly cleaned and standardized.\n",
    "Standardize the data to have a mean of 0 and a standard deviation of 1 for each feature. Standardization is important because PCA is sensitive to the scale of the data.\n",
    "\n",
    "\n",
    "* .Apply PCA:\n",
    "\n",
    "Calculate the covariance matrix of the standardized dataset. The covariance matrix quantifies the relationships between different features.\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix. These eigenvectors represent the principal components (PCs), and the eigenvalues indicate how much variance each PC captures.\n",
    "\n",
    "\n",
    "* . Select the Number of Principal Components:\n",
    "\n",
    "Sort the eigenvalues in descending order. The PCs corresponding to the largest eigenvalues capture the most variance in the data.\n",
    "You can decide how many principal components to keep based on the amount of variance you want to retain. Common choices include retaining enough PCs to capture 95% or 99% of the total variance.\n",
    "\n",
    "* .Transform the Data:\n",
    "\n",
    "Use the selected principal components to transform the original data into a new feature space. Each instance in the dataset is now represented by its values along the new axes (principal components).\n",
    "\n",
    "* . Model Building:\n",
    "\n",
    "Train your stock price prediction model using the reduced-dimension dataset. You can use various regression models, time series models, or machine learning algorithms to build your predictive model.\n",
    "\n",
    "\n",
    "* . Model Evaluation and Tuning:\n",
    "\n",
    "Evaluate the performance of your model using appropriate metrics such as mean squared error (MSE), root mean squared error (RMSE), or others.\n",
    "If the model's performance is not satisfactory, you can experiment with different numbers of principal components or other hyperparameters to improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34177cd0-1bd0-4d75-838b-796366bf2a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772994c-fe11-4b93-a534-0517a00daf97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e1a95-31a8-430a-a365-a0d1321bc70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c765971-f8aa-4a56-b048-00cbb13633cc",
   "metadata": {},
   "source": [
    "## Question - 7\n",
    "ans - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e875d4-8348-4e8b-8d0c-5711eacf34e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "data = [[1], [5], [10], [15], [20]]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02d075-1cc7-4c02-bbab-8d542a3e9c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674dff2-0e8d-4791-b794-504d40165926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02880736-6900-4000-a69b-e43505d3f26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50cd2e70-f2d6-4074-b888-93b6f702991d",
   "metadata": {},
   "source": [
    "## Question - 8\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c65ac5-820c-46f2-9031-14bdd546f8b2",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that can be used to extract the most important features from a dataset. The number of principal components to retain depends on the specific goals of your analysis and the level of variance you want to preserve.\n",
    "\n",
    "Here's how you can determine how many principal components to retain:\n",
    "\n",
    "1. Calculate the covariance matrix of your data.\n",
    "2. Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "3. Sort the eigenvalues in descending order.\n",
    "4. Calculate the cumulative explained variance.\n",
    "\n",
    "You would typically choose to retain a sufficient number of principal components to explain a high percentage of the total variance. For example, if you want to retain 95% of the variance, you would select the smallest number of principal components that collectively explain at least 95% of the variance. This decision is somewhat arbitrary and depends on the specific trade-off between dimensionality reduction and information loss that you are willing to make.\n",
    "\n",
    "Let's assume you perform PCA on your dataset of features [height, weight, age, gender, blood pressure] and find that the cumulative explained variance is as follows:\n",
    "\n",
    "1st principal component explains 60% of the variance.\n",
    "2nd principal component explains 25% of the variance.\n",
    "3rd principal component explains 10% of the variance.\n",
    "4th principal component explains 4% of the variance.\n",
    "5th principal component explains 1% of the variance.\n",
    "\n",
    "In this hypothetical scenario, you might choose to retain the first two principal components, which collectively explain 85% of the variance (60% + 25%). This retains most of the important information while reducing the dimensionality of your data.\n",
    "\n",
    "The specific number of principal components to retain may vary depending on your use case and goals. If you have a specific threshold of explained variance in mind, you can choose the number of principal components that meets that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e78db-36f9-474a-88f0-3456e987856a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
